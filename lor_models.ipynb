{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt             #visualisation\n",
    "import seaborn as sns                       #visualisation\n",
    "%matplotlib inline     \n",
    "sns.set(color_codes=True)\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, classification_report, make_scorer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cell_prefix(df, prefix):\n",
    "    df.index = [prefix + i for i in df.index]\n",
    "    return df\n",
    "\n",
    "#Remove extra quotes from the filenames\n",
    "def extract_cell_name_smartseq(x):\n",
    "    y = x.split(\"_\")\n",
    "    return y[len(y)-2]\n",
    "\n",
    "def get_cell_name_smartseq(file_name):\n",
    "    return extract_cell_name_smartseq(file_name)\n",
    "\n",
    "def convert_indexes_to_cell_names_smartseq(df):\n",
    "    df.index = [get_cell_name_smartseq(x) for x in df.index]\n",
    "    return df\n",
    "\n",
    "def get_cell_hypo_or_norm_smartseq(df_meta, cell_name):\n",
    "    return df_meta[df_meta[\"Cell name\"]==cell_name][\"Condition\"].values[0]\n",
    "\n",
    "def seperate_hypo_and_norm_smartseq(df, df_meta):\n",
    "    df_hypo = df[df.index.map(lambda x: get_cell_hypo_or_norm_smartseq(df_meta, x)==\"Hypo\")]\n",
    "    df_norm = df[df.index.map(lambda x: get_cell_hypo_or_norm_smartseq(df_meta, x)==\"Norm\" or get_cell_hypo_or_norm_smartseq(df_meta, x)==\"Normo\")]\n",
    "    return df_hypo, df_norm\n",
    "\n",
    "def process_df_smartseq(df, df_meta, prefix):\n",
    "    df = convert_indexes_to_cell_names_smartseq(df)\n",
    "    _, df_norm = seperate_hypo_and_norm_smartseq(df,df_meta)\n",
    "    df = add_cell_prefix(df, prefix)\n",
    "    df_norm = add_cell_prefix(df_norm, prefix)\n",
    "    return df, df_norm.index\n",
    "\n",
    "#Remove extra quotes from the filenames\n",
    "def extract_cell_name_dropseq(x):\n",
    "    y = x.split(\"_\")\n",
    "    return y[0]\n",
    "\n",
    "def get_cell_name_dropseq(file_name):\n",
    "    return extract_cell_name_dropseq(file_name)\n",
    "\n",
    "def convert_indexes_to_cell_names_dropseq(df):\n",
    "    df.index = [get_cell_name_dropseq(x) for x in df.index]\n",
    "    return df\n",
    "\n",
    "def get_cell_hypo_or_norm_dropseq(cell_name):\n",
    "    return cell_name.split(\"_\")[-1]\n",
    "\n",
    "def seperate_hypo_and_norm_dropseq(df):\n",
    "    df_hypo = df[df.index.map(lambda x: get_cell_hypo_or_norm_dropseq(x)==\"Hypoxia\")]\n",
    "    df_norm = df[df.index.map(lambda x: get_cell_hypo_or_norm_dropseq(x)==\"Normoxia\")]\n",
    "    return df_hypo, df_norm\n",
    "\n",
    "def process_df_dropseq(df, prefix):\n",
    "    _, df_norm = seperate_hypo_and_norm_dropseq(df)\n",
    "    df = convert_indexes_to_cell_names_dropseq(df)\n",
    "    df_norm = convert_indexes_to_cell_names_dropseq(df_norm)\n",
    "    df = add_cell_prefix(df, prefix)\n",
    "    df_norm = add_cell_prefix(df_norm, prefix)\n",
    "    return df, df_norm.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcf7_smartS_meta = pd.read_csv(\"Data/SmartSeq/MCF7_SmartS_MetaData.tsv\",delimiter=\"\\t\", index_col=0)\n",
    "mcf7_smartS_filn = pd.read_csv(\"Data/SmartSeq/MCF7_SmartS_Filtered_Normalised_3000_Data_train.txt\",delimiter=\" \",index_col=0).T\n",
    "hcc_smartS_meta = pd.read_csv(\"Data/SmartSeq/HCC1806_SmartS_MetaData.tsv\",delimiter=\"\\t\",index_col=0)\n",
    "hcc_smartS_filn = pd.read_csv(\"Data/SmartSeq/HCC1806_SmartS_Filtered_Normalised_3000_Data_train.txt\",delimiter=\" \",index_col=0).T\n",
    "mcf7_dropS_filn = pd.read_csv(\"Data/DropSeq/MCF7_Filtered_Normalised_3000_Data_train.txt\",delimiter=\" \",index_col=0).T\n",
    "hcc_dropS_filn = pd.read_csv(\"Data/DropSeq/HCC1806_Filtered_Normalised_3000_Data_train.txt\",delimiter=\" \",index_col=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcf7_smartS_filn, mcf7_smartS_filn_idx = process_df_smartseq(mcf7_smartS_filn.copy(), mcf7_smartS_meta, \"MCF7_\")\n",
    "hcc_smartS_filn, hcc_smartS_filn_idx = process_df_smartseq(hcc_smartS_filn.copy(), hcc_smartS_meta, \"HCC1806_\")\n",
    "mcf7_dropS_filn, mcf7_dropS_filn_idx = process_df_dropseq(mcf7_dropS_filn, \"MCF7_\")\n",
    "hcc_dropS_filn, hcc_dropS_filn_idx = process_df_dropseq(hcc_dropS_filn, \"HCC1806_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = [mcf7_smartS_filn, hcc_smartS_filn, mcf7_dropS_filn, hcc_dropS_filn]\n",
    "dataset_names = [\"MCF7_SmartSeq\", \"HCC1806_SmartSeq\", \"MCF7_DropSeq\", \"HCC1806_DropSeq\"]\n",
    "datasets = dict(zip(dataset_names, dataset_list))\n",
    "idx_datasets = dict(zip(dataset_names, [mcf7_smartS_filn_idx, hcc_smartS_filn_idx, mcf7_dropS_filn_idx, hcc_dropS_filn_idx]))\n",
    "del dataset_list, dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each dataset in datasets, create a target column which is one if the index is in idx_datasets[dataset] and 0 otherwise\n",
    "for name, df in datasets.items():\n",
    "    df[\"target\"] = df.index.map(lambda x: 1 if x in idx_datasets[name] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of MCF7_SmartSeq dataset: (250, 3001)\n",
      "Length of HCC1806_SmartSeq dataset: (182, 3001)\n",
      "Length of MCF7_DropSeq dataset: (21626, 3001)\n",
      "Length of HCC1806_DropSeq dataset: (14682, 3001)\n"
     ]
    }
   ],
   "source": [
    "for name in datasets:\n",
    "    print(f'Length of {name} dataset: {datasets[name].shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a general Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(X, y, params, pipeline, n_repeats=3, hyperparameter_tuner='grid', n_iter=None, verbose=1, sample_weights=None, probability=True, principal_metric='roc_auc', error_score=np.nan):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=n_repeats, random_state=42)\n",
    "    if probability:\n",
    "        scoring = ['roc_auc', 'accuracy', 'f1', 'precision', 'recall']\n",
    "    else: \n",
    "        scoring = ['accuracy', 'f1', 'precision', 'recall']\n",
    "    principal_metric = principal_metric\n",
    "\n",
    "    if hyperparameter_tuner == 'grid':\n",
    "        search = GridSearchCV(pipeline, params, cv=cv, scoring=scoring, refit=principal_metric, n_jobs=-1, verbose=verbose, error_score=np.nan)\n",
    "    elif hyperparameter_tuner == 'random':\n",
    "        search = RandomizedSearchCV(pipeline, params, cv=cv, scoring=scoring, refit=principal_metric, n_iter=n_iter, n_jobs=-1, verbose=verbose, error_score=np.nan)\n",
    "    elif hyperparameter_tuner == 'bayes':\n",
    "        search = BayesSearchCV(pipeline, params, cv=cv, scoring=scoring, refit=principal_metric, n_iter=n_iter, n_jobs=-1, verbose=verbose, error_score=np.nan)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown hyperparameter tuner: {hyperparameter_tuner}\\n Choose from 'grid', 'random', 'bayes'\")\n",
    "    if sample_weights is not None:\n",
    "        kwargs = {pipeline.steps[-1][0] + '__sample_weight': sample_weights}\n",
    "        result = search.fit(X, y, **kwargs)\n",
    "    else:\n",
    "        result = search.fit(X, y)\n",
    "    best_model = result.best_estimator_\n",
    "\n",
    "    return result, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation_performance(result, principal_metric='roc_auc',probability=True):\n",
    "    print('')\n",
    "    if probability:\n",
    "        scoring = ['roc_auc', 'accuracy', 'f1', 'precision', 'recall']\n",
    "    else: \n",
    "        scoring = ['accuracy', 'f1', 'precision', 'recall']\n",
    "    # order the list such that the principal metric is the first element\n",
    "    scoring.insert(0, scoring.pop(scoring.index(principal_metric)))\n",
    "    for i in scoring:\n",
    "        if i == principal_metric:\n",
    "            print(f'Best {i} score: {result.best_score_}')\n",
    "            print('')\n",
    "        print(f\"Cross-validation {i} scores: {result.cv_results_['mean_test_'+i][result.best_index_]}\\n\")\n",
    "    print(f\"\\nParameters for the best model: \\n{result.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(result, principal_metric='roc_auc'):\n",
    "    # Extract the results into a pandas DataFrame\n",
    "    results = pd.DataFrame(result.cv_results_)\n",
    "    \n",
    "    # Extract parameter names\n",
    "    param_names = [col for col in results.columns if col.startswith('param_')]\n",
    "    \n",
    "    # Create subplots for each parameter\n",
    "    num_params = len(param_names)\n",
    "    fig, axs = plt.subplots(num_params, 1, figsize=(10, num_params * 4))\n",
    "    \n",
    "    # Ensure axs is a 1D array\n",
    "    if num_params == 1:\n",
    "        axs = [axs]\n",
    "    \n",
    "    for i, param in enumerate(param_names):\n",
    "        param_values = results[param]\n",
    "        mean_test_score = results[f'mean_test_{principal_metric}']\n",
    "        \n",
    "        # Create a dataframe for table and compute average scores for unique parameter values\n",
    "        table_data = pd.DataFrame({\n",
    "            param: param_values,\n",
    "            f'mean_test_{principal_metric}': mean_test_score,\n",
    "        })\n",
    "\n",
    "        # Sort parameter values correctly if they are numerical\n",
    "        try:     \n",
    "            table_data[param] =  table_data[param].fillna(-1)   \n",
    "            table_data[param] = table_data[param].astype(float)\n",
    "        except (TypeError, ValueError):\n",
    "            table_data[param] = table_data[param].astype(str)\n",
    "        \n",
    "        grouped_data = table_data.groupby(param).agg({\n",
    "            f'mean_test_{principal_metric}': list,\n",
    "        }).reset_index()\n",
    "        \n",
    "        grouped_data = grouped_data.sort_values(by=param)     \n",
    "        sorted_param_values = grouped_data[param].replace([-1, \"-1\"], \"N/A\")\n",
    "        \n",
    "        # Plot mean test score using boxplot\n",
    "        sns.boxplot(x=sorted_param_values, y=grouped_data[f'mean_test_{principal_metric}'].explode(), ax=axs[i], color='#8EC6FF', width=0.8)\n",
    "        axs[i].set_title(f'Mean {principal_metric} score of {param}')\n",
    "        axs[i].set_xlabel(param)\n",
    "        axs[i].set_ylabel(f'Mean {principal_metric} score')\n",
    "        axs[i].tick_params(axis='x', rotation=20)\n",
    "        axs[i].format_coord = lambda x,y: '%3d, %3d' % (x,y)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test_performance(name, best_model, X_test, y_test, probability=True):\n",
    "    print(f'Evaluating the performance on the test set for {name}:\\n')\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    if probability:\n",
    "        y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f'ROC AUC: {roc_auc}')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'F1: {f1}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "\n",
    "    # display confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "    classes = ('Normoxia', 'Hypoxia')\n",
    "    df_cm = pd.DataFrame(cm, index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "    plt.figure(figsize = (12,7))\n",
    "    sns.heatmap(df_cm, annot=True)\n",
    "    plt.title(f'Confusion matrix for {name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each dataset we create a train-test split\n",
    "X_train, X_test, y_train, y_test = {}, {}, {}, {}\n",
    "for name, df in datasets.items():\n",
    "    X = df.drop(columns=['target'])\n",
    "    y = df['target']\n",
    "    X_train[name], X_test[name], y_train[name], y_test[name] = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST:\n",
    "\n",
    "We first run XGBOOST, a very popular and flexiblle ensemble learning technique based on decision trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_pipe_params(X, y, n_estimators=200, balance=True):\n",
    "    # find total negative and positive samples \n",
    "    total = len(y)\n",
    "    total_positive = np.sum(y==1)\n",
    "    total_negative = total - total_positive\n",
    "\n",
    "    # set the scale pos weight\n",
    "    scale_pos_weight = total_negative / total_positive if balance else 1\n",
    "\n",
    "    # Define the pipeline steps\n",
    "    xgb_pipeline = Pipeline([\n",
    "        ('xgb', XGBClassifier(scale_pos_weight=scale_pos_weight, n_estimators=n_estimators, device='cuda', n_jobs=-1))  \n",
    "    ])\n",
    "\n",
    "    xgb_params = {\n",
    "        'xgb__reg_alpha': [0.0, 1, 10, 100],\n",
    "        'xgb__eta': [0.1, 0.3, 0.5],\n",
    "        'xgb__colsample_bytree': [0.5, 1.0],\n",
    "        'xgb__subsample': [0.5, 1.0],\n",
    "        'xgb__colsample_bylevel': [0.5, 1.0]\n",
    "    }\n",
    "\n",
    "    return xgb_pipeline, xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning MCF7_SmartSeq dataset\n",
      "Tuning HCC1806_SmartSeq dataset\n",
      "Fitting 15 folds for each of 96 candidates, totalling 1440 fits\n",
      "Tuning MCF7_DropSeq dataset\n",
      "Fitting 15 folds for each of 96 candidates, totalling 1440 fits\n"
     ]
    }
   ],
   "source": [
    "# Training the models\n",
    "for name, df in datasets.items():\n",
    "    print(f'Tuning {name} dataset')\n",
    "    if (name == 'MCF7_SmartSeq'):\n",
    "        continue\n",
    "    if (name == 'MCF7_SmartSeq') or (name == 'HCC1806_SmartSeq'):\n",
    "        n_estimators = 100\n",
    "    else:\n",
    "        n_estimators = 1000\n",
    "    pipeline, params = xgb_pipe_params(X_train[name], y_train[name], n_estimators=n_estimators, balance=True)\n",
    "    result, best_model = tune_model(X_train[name], y_train[name], params, pipeline, n_repeats=3, hyperparameter_tuner='grid', n_iter=100, verbose=10, probability=True, principal_metric='roc_auc', error_score=np.nan)\n",
    "\n",
    "    # save the model and the result\n",
    "    model_name = f'xgboost_mkII_{name}'\n",
    "    with open(f'models/{model_name}.pkl', 'wb') as f:\n",
    "        pkl.dump(best_model, f)\n",
    "\n",
    "    result_name = f'xgboost_mkII_result_{name}'\n",
    "    with open(f'results/{result_name}.pkl', 'wb') as f:\n",
    "        pkl.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models in their k-fold cross-validation\n",
    "for name, df in datasets.items():\n",
    "    with open(f'results/xgboost_mkII_result_{name}.pkl', 'rb') as f:\n",
    "        result = pkl.load(f)\n",
    "    model_validation_performance(result, probability=True)\n",
    "    plot_results(result, principal_metric='roc_auc')\n",
    "    print(f'\\nThe training of {name} is done, saving the model\\n\\n')\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the best models\n",
    "for name, df in datasets.items():\n",
    "    # load the model\n",
    "    model_name = f'xgboost_mkII_{name}'\n",
    "    with open(f'models/{model_name}.pkl', 'rb') as f:\n",
    "        best_model = pkl.load(f)\n",
    "    \n",
    "    model_test_performance(name, best_model, X_test[name], y_test[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_pipe_params(X, y, n_estimators=200, balance=True):\n",
    "    # Define the pipeline steps\n",
    "    rf_pipeline = Pipeline([\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', n_estimators=n_estimators, n_jobs=-1))  \n",
    "    ])\n",
    "\n",
    "    rf_params = {\n",
    "        'rf__criterion': ['gini', 'entropy'],\n",
    "        'rf__max_depth': [20, n_estimators*0.3, None],\n",
    "        'rf__min_samples_split': [1, 5, 10],\n",
    "        'rf__boostrap': [True, False]\n",
    "    }\n",
    "    return rf_pipeline, rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the models\n",
    "for name, df in datasets.items():\n",
    "    print(f'Tuning {name} dataset')\n",
    "    if (name == 'MCF7_SmartSeq') or (name == 'HCC1806_SmartSeq'):\n",
    "        n_estimators = 100\n",
    "    else:\n",
    "        n_estimators = 1000\n",
    "    pipeline, params = rf_pipe_params(X_train[name], y_train[name], n_estimators=n_estimators, balance=True)\n",
    "    result, best_model = tune_model(X_train[name], y_train[name], params, pipeline, n_repeats=3, hyperparameter_tuner='grid', n_iter=100, verbose=10, probability=True, principal_metric='roc_auc', error_score=np.nan)\n",
    "\n",
    "    # save the model and the result\n",
    "    model_name = f'rf_{name}'\n",
    "    with open(f'models/{model_name}.pkl', 'wb') as f:\n",
    "        pkl.dump(best_model, f)\n",
    "\n",
    "    result_name = f'rf_result_{name}'\n",
    "    with open(f'results/{result_name}.pkl', 'wb') as f:\n",
    "        pkl.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models in their k-fold cross-validation\n",
    "for name, df in datasets.items():\n",
    "    print(f'Validating ')\n",
    "    with open(f'results/rf_result_{name}.pkl', 'rb') as f:\n",
    "        result = pkl.load(f)\n",
    "    model_validation_performance(result, probability=True)\n",
    "    plot_results(result, principal_metric='roc_auc')\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the best models\n",
    "for name, df in datasets.items():\n",
    "    # load the model\n",
    "    model_name = f'rf_{name}'\n",
    "    with open(f'models/{model_name}.pkl', 'rb') as f:\n",
    "        best_model = pkl.load(f)\n",
    "    \n",
    "    model_test_performance(name, best_model, X_test[name], y_test[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPPORT VECTOR MACHINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_pipe_params(X, y, balance=True):\n",
    "    # Define the pipeline steps\n",
    "    svm_pipeline = Pipeline([\n",
    "        ('svm', SVC(class_weight='balanced'))  \n",
    "    ])\n",
    "\n",
    "    svm_params = {\n",
    "        'svm__C': [0.1, 1, 10, 100],\n",
    "        'svm__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'svm__degree': [2, 5],\n",
    "        'svm__gamma': ['scale', 'auto'],\n",
    "        'svm__coef0': [0.0, 1.0]\n",
    "    }\n",
    "    return svm_pipeline, svm_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning MCF7_SmartSeq dataset\n",
      "Fitting 15 folds for each of 128 candidates, totalling 1920 fits\n",
      "Tuning HCC1806_SmartSeq dataset\n",
      "Fitting 15 folds for each of 128 candidates, totalling 1920 fits\n",
      "Tuning MCF7_DropSeq dataset\n",
      "Fitting 15 folds for each of 128 candidates, totalling 1920 fits\n"
     ]
    }
   ],
   "source": [
    "for name, df in datasets.items():\n",
    "    print(f'Tuning {name} dataset')\n",
    "    if name in ['MCF7_SmartSeq, HCC1806_SmartSeq']:\n",
    "        continue\n",
    "    pipeline, params = svm_pipe_params(X_train[name], y_train[name], balance=True)\n",
    "    result, best_model = tune_model(X_train[name], y_train[name], params, pipeline, n_repeats=3, hyperparameter_tuner='grid', n_iter=100, verbose=10, probability=False, principal_metric='accuracy', error_score=np.nan)\n",
    "\n",
    "    # save the model and the result\n",
    "    model_name = f'svm_{name}'\n",
    "    with open(f'models/{model_name}.pkl', 'wb') as f:\n",
    "        pkl.dump(best_model, f)\n",
    "\n",
    "    result_name = f'svm_result_{name}'\n",
    "    with open(f'results/{result_name}.pkl', 'wb') as f:\n",
    "        pkl.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in datasets.items():\n",
    "    with open(f'results/svm_result_{name}.pkl', 'rb') as f:\n",
    "        result = pkl.load(f)\n",
    "    model_validation_performance(result, probability=False)\n",
    "    print(f'\\nThe training of {name} is done, saving the model\\n\\n')\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in datasets.items():\n",
    "    # load the model\n",
    "    model_name = f'svm_{name}'\n",
    "    with open(f'models/{model_name}.pkl', 'rb') as f:\n",
    "        best_model = pkl.load(f)\n",
    "    \n",
    "    model_test_performance(name, best_model, X_test[name], y_test[name], probability=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
